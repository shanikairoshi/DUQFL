{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanikairoshi/DUQFL/blob/main/DQFL_Genome_IID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdPhLlT9q3Np"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaeXy79ZCl0i",
        "outputId": "c64c1910-4e71-48c4-e713-fac6b2a368b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 31 01:47:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   64C    P8             14W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Your runtime has 56.9 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Us3DV6Sfq6Mw"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "!pip install genomic-benchmarks\n",
        "!pip install qiskit qiskit_machine_learning qiskit_algorithms\n",
        "!pip install qiskit-aer\n",
        "!pip install qiskit\n",
        "!pip install qiskit_machine_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m5M_eDObB6OI"
      },
      "outputs": [],
      "source": [
        "#-------Split data for federated Setting--------#\n",
        "num_epochs = 10 #50\n",
        "max_train_iterations = 100\n",
        "samples_per_epoch=50 #10\n",
        "#backend = Aer.get_backend('aer_simulator')\n",
        "word_size = 40\n",
        "\n",
        "# Configuration variables\n",
        "num_clients = 10\n",
        "num_federated_layers = 5\n",
        "num_deep_unfolding_iterations = 8\n",
        "initial_learning_rate = 0.15\n",
        "meta_learning_rate=1e-4\n",
        "initial_perturbation = 0.15\n",
        "momentum = 0.95\n",
        "gradient_moving_avg = 0\n",
        "\n",
        "# Define federated learning with accuracy tracking\n",
        "num_features = 5\n",
        "global_model_weights, global_model_accuracy = {}, []\n",
        "clients_train_accuracies, clients_test_accuracies = [], []\n",
        "\n",
        "# Define the federated learning parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpAw5S3imZQW",
        "outputId": "85575703-1a72-4e98-d61e-68c0c3386abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/genomic_benchmarks/utils/datasets.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JW0-eTB-rJXvFcglqBo3pFZi1kyIWC3X\n",
            "From (redirected): https://drive.google.com/uc?id=1JW0-eTB-rJXvFcglqBo3pFZi1kyIWC3X&confirm=t&uuid=27590a9b-4a7d-46ab-b018-276905d44751\n",
            "To: /root/.genomic_benchmarks/demo_human_or_worm.zip\n",
            "100%|██████████| 28.9M/28.9M [00:00<00:00, 52.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nuber of samples in the test set: 25000\n",
            "Nuber of samples in the test set: 75000\n",
            "First sample int the data_set variable: \n",
            "('AATGGATAATTTTTTTATTTTTTCAACCAGGAGAATTTCGGTGCAACGAAAACGCATGTGCATTTTGATCTACCTGCATTGCGACAAGCCTTACAGCAACAACGAAGACTGGTCGATTCAAGCTGAAGTACATCTTATTTTGGTTTCAGACACAGGAAGAACATCTACTGATAGGGTCACTCACATTTTCGAAAAACCAG', 0)\n",
            "\n",
            "First 5 samples in the word_combinations dict.\n",
            "AATGGATAATTTTTTTATTTTTTCAACCAGGAGAATTTCG 1\n",
            "ATGGATAATTTTTTTATTTTTTCAACCAGGAGAATTTCGG 2\n",
            "TGGATAATTTTTTTATTTTTTCAACCAGGAGAATTTCGGT 3\n",
            "GGATAATTTTTTTATTTTTTCAACCAGGAGAATTTCGGTG 4\n",
            "GATAATTTTTTTATTTTTTCAACCAGGAGAATTTCGGTGC 5\n",
            "First 5 samples of encoded data:\n",
            "First 5 samples of encoded shuffled data:\n",
            "First 5 samples of scaled encoded shuffled data:\n",
            "Length of np_train_data: 15000\n",
            "Length of np_test_data: 5000\n",
            "Client 0 Test Data Length: 500\n",
            "Client 1 Test Data Length: 500\n",
            "Client 2 Test Data Length: 500\n",
            "Client 3 Test Data Length: 500\n",
            "Client 4 Test Data Length: 500\n",
            "Client 5 Test Data Length: 500\n",
            "Client 6 Test Data Length: 500\n",
            "Client 7 Test Data Length: 500\n",
            "Client 8 Test Data Length: 500\n",
            "Client 9 Test Data Length: 500\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n",
        "import time\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
        "from qiskit.primitives import BackendSampler\n",
        "from functools import partial\n",
        "from qiskit_aer import Aer\n",
        "\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from qiskit_algorithms.utils import algorithm_globals # Import algorithm_globals\n",
        "\n",
        "# Set random seed for reproducibility using algorithm_globals\n",
        "algorithm_globals.random_seed = 42  # Set seed globally\n",
        "\n",
        "\n",
        "test_set = DemoHumanOrWorm(split='test', version=0)\n",
        "train_set = DemoHumanOrWorm(split='train', version=0)\n",
        "\n",
        "data_set = train_set\n",
        "# data_set = train_set + test_set\n",
        "len(data_set)\n",
        "\n",
        "\n",
        "print(f\"Nuber of samples in the test set: {len(test_set)}\")\n",
        "print(f\"Nuber of samples in the test set: {len(train_set)}\")\n",
        "\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n",
        "\n",
        "test_set = DemoHumanOrWorm(split='test', version=0)\n",
        "train_set = DemoHumanOrWorm(split='train', version=0)\n",
        "\n",
        "data_set = train_set\n",
        "# data_set = train_set + test_set\n",
        "len(data_set)\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "word_combinations = defaultdict(int)\n",
        "iteration = 1\n",
        "for text, _ in data_set:\n",
        "    for i in range(len(text)):\n",
        "        word = text[i:i+word_size]\n",
        "        if word_combinations.get(word) is None:\n",
        "          word_combinations[word] = iteration\n",
        "          iteration += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"First sample int the data_set variable: \")\n",
        "print(data_set[0])\n",
        "\n",
        "print(\"\\nFirst 5 samples in the word_combinations dict.\")\n",
        "for key, value in list(word_combinations.items())[:5]:\n",
        "    print(key, value)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# Preprocess the training set\n",
        "np_data_set = []\n",
        "for i in range(len(data_set)):\n",
        "    sequence, label = data_set[i]\n",
        "    sequence = sequence.strip()  # Remove any leading/trailing whitespace\n",
        "    words = [sequence[i:i + word_size] for i in range(0, len(sequence), word_size)]  # Split the sequence into 4-letter words\n",
        "    int_sequence = np.array([word_combinations[word] for word in words])\n",
        "    data_point = {'sequence': int_sequence, 'label': label}\n",
        "    np_data_set.append(data_point)\n",
        "\n",
        "\n",
        "print(\"First 5 samples of encoded data:\")\n",
        "np_data_set[:5]\n",
        "\n",
        "\n",
        "np.random.shuffle(np_data_set)\n",
        "print(\"First 5 samples of encoded shuffled data:\")\n",
        "np_data_set[:5]\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sequences = np.array([item['sequence'] for item in np_data_set])\n",
        "sequences = np.vstack(sequences)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "sequences_scaled = scaler.fit_transform(sequences)\n",
        "\n",
        "for i, item in enumerate(np_data_set):\n",
        "    item['sequence'] = sequences_scaled[i]\n",
        "\n",
        "print(\"First 5 samples of scaled encoded shuffled data:\")\n",
        "np_data_set[:5]\n",
        "\n",
        "\n",
        "np_train_data = np_data_set[:15000]\n",
        "np_test_data = np_data_set[-5000:]\n",
        "\n",
        "print(f\"Length of np_train_data: {len(np_train_data)}\")\n",
        "print(f\"Length of np_test_data: {len(np_test_data)}\")\n",
        "\n",
        "test_sequences = [data_point[\"sequence\"] for data_point in np_test_data]\n",
        "test_labels = [data_point[\"label\"] for data_point in np_test_data]\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "#---------------------------------------\n",
        "\n",
        "\n",
        "class Client:\n",
        "   def __init__(self, data, test_data):  # Add test_data to __init__\n",
        "        self.data = data\n",
        "        self.test_data = test_data  # Store test_data as an attribute\n",
        "        self.models = []\n",
        "        self.train_scores = []\n",
        "        self.test_scores = []\n",
        "        self.primary_model = None\n",
        "\n",
        "def split_dataset(num_clients, num_epochs, samples_per_epoch):\n",
        "    clients = []\n",
        "    # Split test data across clients\n",
        "    test_samples_per_client = len(np_test_data) // num_clients\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client_data = []\n",
        "        for j in range(num_epochs):\n",
        "            start_idx = (i * num_epochs * samples_per_epoch) + (j * samples_per_epoch)\n",
        "            end_idx = (i * num_epochs * samples_per_epoch) + ((j + 1) * samples_per_epoch)\n",
        "            client_data.append(np_train_data[start_idx:end_idx])\n",
        "\n",
        "        # Assign a subset of the test data to each client\n",
        "        test_start_idx = i * test_samples_per_client\n",
        "        test_end_idx = (i + 1) * test_samples_per_client\n",
        "        client_test_data = np_test_data[test_start_idx:test_end_idx]\n",
        "\n",
        "        # Create Client instance with both train and test data\n",
        "        clients.append(Client(client_data, client_test_data))\n",
        "\n",
        "    return clients\n",
        "\n",
        "clients = split_dataset(num_clients, num_epochs, samples_per_epoch)\n",
        "\n",
        "# Verify test data distribution across clients\n",
        "for index, client in enumerate(clients):\n",
        "    print(f\"Client {index} Test Data Length: {len(client.test_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIg_mUrp3_2J"
      },
      "source": [
        "Data Load and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3OTrftC6uZd_"
      },
      "outputs": [],
      "source": [
        "def split_dataset_for_epochs(num_clients, num_epochs, train_data, test_data, samples_per_epoch):\n",
        "    \"\"\"\n",
        "    Split the dataset across multiple epochs and clients.\n",
        "\n",
        "    Args:\n",
        "        num_clients (int): Number of clients.\n",
        "        num_epochs (int): Number of epochs.\n",
        "        train_data (list): List of training data points.\n",
        "        test_data (list): List of test data points.\n",
        "        samples_per_epoch (int): Number of samples per epoch.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of Client objects with assigned data for each epoch.\n",
        "    \"\"\"\n",
        "    clients = []\n",
        "\n",
        "    # Split the training data across epochs and clients\n",
        "    train_samples_per_client = len(train_data) // num_clients\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client_data_for_epochs = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            start_idx = (i * num_epochs * samples_per_epoch) + (epoch * samples_per_epoch)\n",
        "            end_idx = (i * num_epochs * samples_per_epoch) + ((epoch + 1) * samples_per_epoch)\n",
        "            client_data_for_epochs.append(train_data[start_idx:end_idx])\n",
        "\n",
        "        # Assign test data to each client\n",
        "        test_samples_per_client = len(test_data) // num_clients\n",
        "        test_start_idx = i * test_samples_per_client\n",
        "        test_end_idx = (i + 1) * test_samples_per_client\n",
        "        client_test_data = test_data[test_start_idx:test_end_idx]\n",
        "\n",
        "        # Create a Client instance with epoch-specific data\n",
        "        clients.append(Client(client_data_for_epochs, client_test_data))\n",
        "\n",
        "    return clients\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clients = split_dataset_for_epochs(num_clients, num_epochs, np_train_data, np_test_data, samples_per_epoch)\n"
      ],
      "metadata": {
        "id": "wxN0urQP5FZ-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vHHwtTUF65Gp"
      },
      "outputs": [],
      "source": [
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "from qiskit_algorithms.utils import algorithm_globals\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import log_loss\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Callback function to capture the loss values\n",
        "objective_func_vals = []  # Global list to store loss values\n",
        "learning_rates = []\n",
        "perturbations = []\n",
        "# Data structure for tracking per-client, per-layer objective function values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o0CaYcz9FQw7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os  # For handling directories\n",
        "\n",
        "# Define the directory to save the plots\n",
        "output_dir = \"federated_round_plots\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "# Initialize a global variable to track the round number\n",
        "current_round = 1\n",
        "\n",
        "# Callback for visualization, gradient smoothing, and learning rate adjustment in deep unfolding\n",
        "def deep_unfolding_learning_rate_adjustment(parameters, obj_func_eval, gradients=None,round_number=0):\n",
        "    global gradient_moving_avg, learning_rates, perturbations,current_round\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Save the objective function value for visualization\n",
        "    objective_func_vals.append(obj_func_eval)\n",
        "\n",
        "    # If gradients are provided, smooth the gradient using momentum\n",
        "    if gradients is not None:\n",
        "        gradient_moving_avg = momentum * gradient_moving_avg + (1 - momentum) * gradients  # Apply moving average\n",
        "        delta_lr = 0.05 * gradient_moving_avg  # Adjust learning rate based on the smoothed gradient\n",
        "        delta_perturbation = 0.1 * gradient_moving_avg  # Adjust perturbation based on the same gradient\n",
        "    else:\n",
        "        delta_lr = 0  # No gradient info available in this iteration\n",
        "        delta_perturbation = 0\n",
        "\n",
        "    # Update learning rate and perturbation\n",
        "    if len(learning_rates) > 0:\n",
        "        new_lr = max(0.001, learning_rates[-1] + delta_lr)  # Ensure learning rate is positive and non-zero\n",
        "        new_perturbation = max(0.001, perturbations[-1] + delta_perturbation)  # Ensure perturbation is positive\n",
        "    else:\n",
        "        new_lr = initial_learning_rate\n",
        "        new_perturbation = initial_perturbation\n",
        "\n",
        "    learning_rates.append(new_lr)\n",
        "    perturbations.append(new_perturbation)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Visualization of learning rate and perturbation\n",
        "    plt.figure(figsize=(10, 12))  # Adjust figure size for better spacing\n",
        "\n",
        "    # Plot Objective Function Value\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals, label=\"Objective Function Value\", color='blue')\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective Function Value\")\n",
        "    plt.title(\"Objective Function Over Iterations\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)  # Add grid for better readability\n",
        "\n",
        "    # Plot Learning Rate\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(range(len(learning_rates)), learning_rates, label=\"Learning Rate\", color='green')\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Learning Rate\")\n",
        "    plt.title(\"Learning Rate Over Iterations\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot Perturbation\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(range(len(perturbations)), perturbations, label=\"Perturbation\", color='red')\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Perturbation\")\n",
        "    plt.title(\"Perturbation Over Iterations\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout(pad=3.0)  # Add padding between subplots\n",
        "    # Save the plot after each federated round\n",
        "    #plot_filename = os.path.join(output_dir, f\"federated_round_{current_round}.png\")\n",
        "    #plt.savefig(plot_filename)  # Save the figure\n",
        "    #plt.show()\n",
        "    plt.close()  # Close the plot to free memory\n",
        "\n",
        "    # Increment the round number for the next call\n",
        "    current_round += 1\n",
        "\n",
        "\n",
        "# Define the SPSA callback to capture gradients and update learning rate and perturbation dynamically\n",
        "def spsa_callback(nfev, parameters, obj_func_eval, stepsize, accept):\n",
        "    # Assuming `stepsize` contains gradient information or its approximation\n",
        "    gradients = stepsize\n",
        "    deep_unfolding_learning_rate_adjustment(parameters, obj_func_eval, gradients)\n",
        "\n",
        "# Custom SPSA optimizer with learnable learning rate and perturbation\n",
        "class LearnableLRPerturbationSPSA(SPSA):\n",
        "    def __init__(self, initial_lr=1e-4, initial_perturbation=0.01, lr_alpha=0.1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.lr = initial_lr  # Initial learning rate\n",
        "        self.perturbation = initial_perturbation  # Initial perturbation\n",
        "        self.lr_alpha = lr_alpha  # Learning rate and perturbation update speed\n",
        "\n",
        "    def _update_learning_rate_and_perturbation(self, gradient, obj_func_eval):\n",
        "        \"\"\"\n",
        "        Update both learning rate and perturbation based on gradient and objective function evaluation.\n",
        "        The learning rate increases if the objective function improves and decreases otherwise.\n",
        "        \"\"\"\n",
        "        # Use the gradient sign to determine if we should increase or decrease\n",
        "        grad_lr = np.sign(np.mean(gradient))  # Average gradient sign across parameters\n",
        "\n",
        "        if grad_lr > 0:  # Objective function is improving\n",
        "            self.lr += self.lr_alpha * abs(grad_lr)  # Increase learning rate\n",
        "            self.perturbation += self.lr_alpha * abs(grad_lr)  # Increase perturbation\n",
        "        else:  # Objective function is getting worse\n",
        "            self.lr -= self.lr_alpha * abs(grad_lr)  # Decrease learning rate\n",
        "            self.perturbation -= self.lr_alpha * abs(grad_lr)  # Decrease perturbation\n",
        "\n",
        "        # Ensure both learning rate and perturbation are positive\n",
        "        self.lr = max(0.001, self.lr)\n",
        "        self.perturbation = max(0.001, self.perturbation)\n",
        "\n",
        "    def step(self, gradient, obj_func_eval):\n",
        "        \"\"\"\n",
        "        Perform optimization step for both parameters, learning rate, and perturbation.\n",
        "        Use the objective function evaluation to dynamically adjust learning rate and perturbation.\n",
        "        \"\"\"\n",
        "        self._update_learning_rate_and_perturbation(gradient, obj_func_eval)\n",
        "        return super().step(gradient)  # Perform SPSA step for parameters\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the optimizer state (learning rates, perturbations, and gradient moving averages) for the next round.\n",
        "        \"\"\"\n",
        "        self.lr = initial_learning_rate\n",
        "        self.perturbation = initial_perturbation\n",
        "        self.gradient_moving_avg = 0  # Reset the moving average of the gradient\n",
        "        learning_rates.clear()  # Reset the learning rates history\n",
        "        perturbations.clear()  # Reset the perturbations history\n",
        "        objective_func_vals.clear()  # Clear the objective function history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X46XXHW1s4tR"
      },
      "outputs": [],
      "source": [
        "# Create optimizer with learnable learning rate and perturbation\n",
        "spsa_optimizer = LearnableLRPerturbationSPSA(\n",
        "      maxiter=50, learning_rate=initial_learning_rate, perturbation=initial_perturbation, callback=spsa_callback, lr_alpha=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-3Rhf0Ft7CI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2b694d-c12c-43da-a5fa-2cf25414e054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#======================================================\n",
        "# Initialize QNN model\n",
        "def initialize_model(num_features,initial_params):\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "    qc = feature_map.compose(ansatz)\n",
        "\n",
        "    # Create optimizer with learnable learning rate and perturbation\n",
        "    spsa_optimizer = LearnableLRPerturbationSPSA(\n",
        "      maxiter=20, learning_rate=initial_learning_rate, perturbation=initial_perturbation, callback=spsa_callback, lr_alpha=0.01\n",
        ")\n",
        "    def parity(x):\n",
        "        return \"{:b}\".format(x).count(\"1\") % 2\n",
        "\n",
        "    sampler_qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        interpret=parity,\n",
        "        output_shape=2,\n",
        "        input_params=feature_map.parameters,\n",
        "        weight_params=ansatz.parameters\n",
        "    )\n",
        "\n",
        "\n",
        "    # Define the neural network classifier\n",
        "    qnn_classifier = NeuralNetworkClassifier(\n",
        "      neural_network=sampler_qnn,\n",
        "      optimizer=spsa_optimizer,\n",
        "      loss='squared_error',\n",
        "      initial_point=initial_params,  # Initialize with the starting parameters\n",
        ")\n",
        "\n",
        "\n",
        "    return qnn_classifier\n",
        "\n",
        "#=====================================================\n",
        "from google.colab import drive\n",
        "import csv\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Define the save path in Google Drive\n",
        "csv_file = '/content/drive/My Drive/DQFL_Genome_IID_10clients_31_01_2025.csv'\n",
        "\n",
        "# Step 3: Define headers for the CSV\n",
        "headers = [\"Federated Round\", \"Client Number\", \"Iteration\", \"Objective Function Value\",\n",
        "           \"Training Accuracy\", \"Test Accuracy\", \"Learning Rate\", \"Perturbation\"]\n",
        "\n",
        "# Open the CSV file and write headers if it's the first time writing to the file\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(headers)\n",
        "\n",
        "# Example of saving results for each federated round and client\n",
        "def save_results(federated_round, client_id, iteration, obj_func_val, train_acc, test_acc, learning_rate, perturbation):\n",
        "    with open(csv_file, mode='a', newline='') as file:  # Open file in append mode\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([federated_round, client_id, iteration, obj_func_val, train_acc, test_acc, learning_rate, perturbation])\n",
        "#=====================================================\n",
        "# Federated learning loop per client\n",
        "def train_qnn_model(client_data, client_test_data, model=None, client_id=None, layer=None):\n",
        "\n",
        "    global learning_rates, perturbations, objective_func_vals\n",
        "    print(\"Client Data Structure:\")  # Add this line to print the structure\n",
        "    print(client_data)                # This line prints the actual data\n",
        "    print(type(client_data))           # This line prints the data type\n",
        "    #num_features = client_data[0][\"sequence\"].shape[0]\n",
        "\n",
        "    #initial_params = np.random.rand(RealAmplitudes(client_data.shape[1], reps=4).num_parameters)  # Initialize params\n",
        "    initial_params = np.random.rand(RealAmplitudes(len(client_data[0][\"sequence\"]), reps=3).num_parameters)\n",
        "\n",
        "    if model is None:\n",
        "        model = initialize_model(num_features, initial_params)\n",
        "\n",
        "    train_sequences = np.array([data_point[\"sequence\"] for data_point in client_data])\n",
        "    train_labels = np.array([data_point[\"label\"] for data_point in client_data])\n",
        "    test_sequences = np.array([data_point[\"sequence\"] for data_point in client_test_data])\n",
        "    test_labels = np.array([data_point[\"label\"] for data_point in client_test_data])\n",
        "\n",
        "    train_accuracies, test_accuracies, total_time = [], [], 0\n",
        "\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    # Deep Unfolding with multiple iterations\n",
        "    # Continue training with learned weights and adjust learning rate based on performance and gradients.\n",
        "    total_time = 0\n",
        "    current_params = initial_params  # Start with the initial parameters\n",
        "\n",
        "    for i in range(num_deep_unfolding_iterations):\n",
        "        print(\"\\n\")\n",
        "        print(f\"Deep Unfolding Iteration {i+1}/{num_deep_unfolding_iterations}\")\n",
        "        start_time = time.time()\n",
        "        model.fit(train_sequences, train_labels)\n",
        "        end_time = time.time()\n",
        "        total_time += end_time - start_time\n",
        "\n",
        "        # After training, retrieve the updated parameters from the optimizer\n",
        "        current_params = model.weights\n",
        "        print(f\"Trained parameters after iteration {i+1}: {current_params}\")\n",
        "\n",
        "        # Store final weights and learning rate for next round\n",
        "        final_learning_rate = learning_rates[-1]\n",
        "        final_perturbation = perturbations[-1]\n",
        "\n",
        "        # Evaluate the model performance\n",
        "        train_accuracy = model.score(train_sequences, train_labels)\n",
        "        test_accuracy = model.score(test_sequences, test_labels)\n",
        "\n",
        "        # Store accuracies for future reference\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "\n",
        "        # Write the results to the CSV file\n",
        "        save_results(layer, client_id, i+1, objective_func_vals[-1], train_accuracy * 100, test_accuracy * 100, final_learning_rate, final_perturbation)\n",
        "\n",
        "        #with open(csv_file, mode='a', newline='') as file:\n",
        "          #writer = csv.writer(file)\n",
        "         #writer.writerow([i+1, objective_func_vals[-1], train_accuracy * 100, test_accuracy * 100, final_learning_rate, final_perturbation])\n",
        "\n",
        "        # Update the learning rate for the next iteration based on gradients from SPSA\n",
        "        spsa_optimizer.learning_rate = learning_rates[-1]\n",
        "        model.initial_point = current_params\n",
        "\n",
        "        # Log performance\n",
        "        print(f\"Iteration {i+1} - Learning Rate: {final_learning_rate:.6f}\")\n",
        "        print(f\"Iteration {i+1} - Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "        print(f\"Iteration {i+1} - Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "    return model, train_accuracy, train_accuracy, total_time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGhtrwV-DDHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step to empty the CSV file before starting a new run\n",
        "def clear_csv_file():\n",
        "    \"\"\"\n",
        "    Clears the CSV file by overwriting it with headers or leaving it blank.\n",
        "    \"\"\"\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Uncomment the next line to write headers for the new run\n",
        "        writer.writerow(headers)\n",
        "        # Leave it blank if you prefer not to include headers\n",
        "        # pass\n",
        "\n"
      ],
      "metadata": {
        "id": "AkLnHZoPTwB6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zpK0oXUHzPtm"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, test_sequences, test_labels):\n",
        "    \"\"\"\n",
        "    Evaluate the accuracy of the given model on the test dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model: The trained model to evaluate.\n",
        "        num_features: The number of features in each data sample.\n",
        "        test_sequences: A list or array of test input data (features).\n",
        "        test_labels: A list or array of true labels corresponding to the test data.\n",
        "\n",
        "    Returns:\n",
        "        float: The accuracy of the model as a percentage.\n",
        "    \"\"\"\n",
        "    test_accuracy = model.score(test_sequences, test_labels)\n",
        "    return test_accuracy\n",
        "\n",
        "# Function to extract numerical values of parameters\n",
        "def extract_param_values(model):\n",
        "    #param_values = []\n",
        "    # Loop through each parameter in the circuit and get its bound value\n",
        "    # Retrieve the circuit from the neural network\n",
        "    circuit = model.neural_network.circuit\n",
        "\n",
        "    # Extract the parameter values bound to the circuit\n",
        "    # Use enumerate to get both index and parameter\n",
        "    param_values = {param: circuit.parameters[i] for i, param in enumerate(circuit.parameters)}\n",
        "    return param_values\n",
        "#def set_param_values(model, param_values):\n",
        "    # Retrieve the circuit from the neural network\n",
        "    #circuit = model.neural_network.circuit\n",
        "\n",
        "    # Use assign_parameters to update the parameter values\n",
        "    #circuit.assign_parameters(param_values, inplace=True)\n",
        "# Function to set numerical values of parameters back into the circuit\n",
        "def set_param_values(model, param_values):\n",
        "    # Assign the averaged parameter values back to the circuit\n",
        "    parameter_dict = {param: value for param, value in zip(model.neural_network.circuit.parameters, param_values)}\n",
        "    model.neural_network.circuit.assign_parameters(parameter_dict)\n",
        "\n",
        "\n",
        "# Manually average the numerical values of the parameters across clients\n",
        "def manual_average_weights(epoch_weights):\n",
        "    # Initialize a list to store the summed weights (initialize with zeros)\n",
        "    num_weights = len(epoch_weights[0])  # Number of weights in the model\n",
        "    num_clients = len(epoch_weights)  # Number of clients\n",
        "\n",
        "    # Initialize sum of weights to zero (assuming NumPy array or list of weights)\n",
        "    summed_weights = np.zeros(num_weights)\n",
        "\n",
        "    # Sum the weights from all clients\n",
        "    for client_weights in epoch_weights:\n",
        "        summed_weights += np.array(client_weights)\n",
        "\n",
        "    # Compute the average by dividing the summed weights by the number of clients\n",
        "    average_weights = summed_weights / num_clients\n",
        "\n",
        "    return average_weights\n",
        "\n",
        "def create_model_with_weights(weights):\n",
        "    initial_params = np.random.rand(RealAmplitudes(num_features, reps=3).num_parameters)\n",
        "    model = initialize_model(num_features,weights)\n",
        "    #set_param_values(model, weights)  # Assign global weights to the model\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MuhZZtnnzmV5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to save accuracies to CSV\n",
        "def save_accuracies_to_csv(global_accuracies, clients_train_accuracies, clients_test_accuracies, filename='accuracies.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the header row\n",
        "        header = ['Epoch', 'Global Accuracy']\n",
        "        for i in range(len(clients_train_accuracies[0])):  # Assuming all clients have the same number of records\n",
        "            header.append(f'Client {i} Train Accuracy')\n",
        "            header.append(f'Client {i} Test Accuracy')\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # Write the accuracy data for each epoch\n",
        "        for epoch in range(len(global_accuracies)):\n",
        "            row = [epoch, global_accuracies[epoch]]  # Start with epoch and global accuracy\n",
        "            for client_index in range(len(clients_train_accuracies[epoch])):\n",
        "                row.append(clients_train_accuracies[epoch][client_index])  # Add train accuracy for client\n",
        "                row.append(clients_test_accuracies[epoch][client_index])   # Add test accuracy for client\n",
        "            writer.writerow(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ez5A1HJ_DS5y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def plot_label_distribution(clients):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for idx, client in enumerate(clients):\n",
        "        labels = [sample['label'] for epoch in client.data for sample in epoch]\n",
        "        label_counts = Counter(labels)\n",
        "        labels_sorted = sorted(label_counts.keys())\n",
        "        counts = [label_counts[label] for label in labels_sorted]\n",
        "\n",
        "        plt.bar(\n",
        "            [str(label) + f\"_C{idx}\" for label in labels_sorted],\n",
        "            counts,\n",
        "            label=f'Client {idx}'\n",
        "        )\n",
        "\n",
        "    plt.xlabel(\"Label per Client\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Label Distribution per Client (Training Data)\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_label_distribution(clients)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "3SPoEA6uDQmh",
        "outputId": "34c9d142-3ffd-4277-b472-a636571846cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a global list to track global loss\n",
        "global_loss_per_round = []\n",
        "\n",
        "# Display information about the data assigned to each client, including epoch-wise splits\n",
        "for idx, client in enumerate(clients):\n",
        "    print(f\"Client {idx + 1}:\")\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"  Epoch {epoch + 1}: Train data samples: {len(client.data[epoch])}\")\n",
        "    print(f\"  Test data samples: {len(client.test_data)}\")\n",
        "\n",
        "# Display information about the data assigned to each client\n",
        "for idx, client in enumerate(clients):\n",
        "    print(f\"Client {idx + 1}:\")\n",
        "    print(f\"  Train data epochs: {len(client.data)}\")\n",
        "    print(f\"  Test data samples: {len(client.test_data)}\")\n",
        "\n",
        "    # Accessing the number of features in a sequence\n",
        "    if client.data:\n",
        "        num_features=client.data[0][0]['sequence'].shape[0]  # Access first data point of epoch 0\n",
        "        #num_features = client.data[0]['sequence'].shape[0]\n",
        "        print(f\"  Number of features in a sequence: {num_features}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s89UEJBz5YQr",
        "outputId": "12cc84e9-331d-4701-c9df-098a76b71d9c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 500\n",
            "Client 2:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 500\n",
            "Client 3:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 500\n",
            "Client 4:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 500\n",
            "Client 5:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 500\n",
            "Client 6:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 500\n",
            "Client 7:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 500\n",
            "Client 8:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 500\n",
            "Client 9:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 500\n",
            "Client 10:\n",
            "  Epoch 1: Train data samples: 50\n",
            "  Epoch 2: Train data samples: 50\n",
            "  Epoch 3: Train data samples: 50\n",
            "  Epoch 4: Train data samples: 50\n",
            "  Epoch 5: Train data samples: 50\n",
            "  Epoch 6: Train data samples: 50\n",
            "  Epoch 7: Train data samples: 50\n",
            "  Epoch 8: Train data samples: 50\n",
            "  Epoch 9: Train data samples: 50\n",
            "  Epoch 10: Train data samples: 50\n",
            "  Test data samples: 500\n",
            "Client 1:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 500\n",
            "  Number of features in a sequence: 5\n",
            "Client 2:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 500\n",
            "  Number of features in a sequence: 5\n",
            "Client 3:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 500\n",
            "  Number of features in a sequence: 5\n",
            "Client 4:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 500\n",
            "  Number of features in a sequence: 5\n",
            "Client 5:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 500\n",
            "  Number of features in a sequence: 5\n",
            "Client 6:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 500\n",
            "  Number of features in a sequence: 5\n",
            "Client 7:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 500\n",
            "  Number of features in a sequence: 5\n",
            "Client 8:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 500\n",
            "  Number of features in a sequence: 5\n",
            "Client 9:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 500\n",
            "  Number of features in a sequence: 5\n",
            "Client 10:\n",
            "  Train data epochs: 10\n",
            "  Test data samples: 500\n",
            "  Number of features in a sequence: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTt4DwZj7Is9"
      },
      "source": [
        "Federated Learning Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmYmJR4_7Hux",
        "outputId": "c49a1f6c-3edb-4b9b-f847-44b478a1d524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained parameters after iteration 5: [-0.04043714  0.92695246  0.05203963  0.75213673  0.31705723  0.50573133\n",
            "  0.78416876  0.86772278  0.52225977  0.56379344  0.29861483  0.0512171\n",
            "  0.95620072  0.4975455   1.27945452  1.03199669  0.06639752  1.01297834\n",
            "  0.29275026  1.12571603]\n",
            "Iteration 5 - Learning Rate: 0.380876\n",
            "Iteration 5 - Training Accuracy: 72.00%\n",
            "Iteration 5 - Test Accuracy: 67.80%\n",
            "\n",
            "\n",
            "Deep Unfolding Iteration 6/8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def reset_state():\n",
        "    # Reset the objective value, learning rate, and perturbation after each client\n",
        "    global objective_func_vals, learning_rates, perturbations\n",
        "    objective_func_vals = []  # Reset objective values\n",
        "    learning_rates = []  # Reset learning rates\n",
        "    perturbations = []  # Reset perturbations\n",
        "# Function to reset callback graph state after each round\n",
        "def reset_callback_graph():\n",
        "    global gradient_moving_avg, learning_rates, perturbations\n",
        "\n",
        "    # Reset the state variables to start fresh for the next round\n",
        "    gradient_moving_avg = np.zeros_like(gradient_moving_avg)  # Reset gradient moving average\n",
        "    learning_rates = [initial_learning_rate]  # Reset learning rates list to initial value\n",
        "    perturbations = [initial_perturbation]  # Reset perturbations list to initial value\n",
        "import csv\n",
        "\n",
        "# Path to store the best client's data\n",
        "best_client_csv_file = '/content/drive/My Drive/Best_Client_DQFL_Genome_IID_10Clients_31_03_2025.csv'\n",
        "\n",
        "# Write headers to the best client CSV file\n",
        "best_headers = [\"Federated Round\", \"Client Number\"]\n",
        "\n",
        "with open(best_client_csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(best_headers)\n",
        "\n",
        "# Function to update the best client data\n",
        "def save_best_client_results(federated_round,best_client_index):\n",
        "    \"\"\"\n",
        "    Save the best client's data to a separate CSV file.\n",
        "    :param best_data: Dictionary containing the best client's data.\n",
        "    \"\"\"\n",
        "    with open(best_client_csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([federated_round,\n",
        "           best_client_index\n",
        "\n",
        "        ])\n",
        "# Clear the CSV file for a new run\n",
        "clear_csv_file()\n",
        "\n",
        "# Wrap the epoch loop with tqdm\n",
        "for epoch in tqdm(range(num_federated_layers), desc=\"Training Progress\", leave =True):\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_train_accuracies, epoch_test_accuracies = [], []\n",
        "    best_client_index = -1\n",
        "    best_client_accuracy = -1\n",
        "    best_client_model = None\n",
        "    print(\"\\n\")\n",
        "    print(f\"Fed_Epoch: {epoch}\")\n",
        "    round_losses = []  # Track individual client losses for this round\n",
        "\n",
        "    for index, client in enumerate(clients):\n",
        "        print(\"\\n\")\n",
        "        print(f\"Fed_Epoch {epoch}, Client {index + 1}:\")\n",
        "        reset_state()\n",
        "\n",
        "        try:\n",
        "            # Ensure you're using the correct index for data\n",
        "            current_data = client.data[epoch]  # This assumes data is structured in epochs\n",
        "            print(f\"Training data for epoch {epoch}: {len(current_data)}\")\n",
        "        except IndexError:\n",
        "            print(f\"No data available for epoch {epoch} for Client {index + 1}\")\n",
        "            continue  # Skip this client for the current epoch\n",
        "\n",
        "        model, train_score, test_score, train_time = train_qnn_model(\n",
        "            client.data[epoch],\n",
        "            client.test_data,\n",
        "            client_id=index,\n",
        "            layer=epoch,\n",
        "        )\n",
        "\n",
        "        epoch_train_accuracies.append(train_score)\n",
        "        epoch_test_accuracies.append(test_score)\n",
        "\n",
        "        # Fetch the client's loss (assumes train_qnn_model returns it)\n",
        "        current_loss = objective_func_vals[-1]  # Fetch latest loss\n",
        "        round_losses.append(current_loss)\n",
        "        # Calculate global loss for the current round as the average of client losses\n",
        "\n",
        "\n",
        "        # Check if this client has the best accuracy so far\n",
        "        if test_score > best_client_accuracy:\n",
        "            best_client_accuracy = test_score\n",
        "            best_client_index = index\n",
        "            best_client_model = model  # Directly store the best client's model\n",
        "\n",
        "    global_loss = sum(round_losses) / len(round_losses)\n",
        "    global_loss_per_round.append(global_loss)  # Store the global loss\n",
        "\n",
        "    print(f\"Global Loss for Round {epoch}: {global_loss}\")\n",
        "\n",
        "    save_best_client_results(epoch,best_client_index)  # Save to best client CSV\n",
        "    print(f\"Best client for epoch {epoch} is Client {best_client_index + 1} with test accuracy {best_client_accuracy:.2f}\")\n",
        "\n",
        "    # Treat the best client's model as the global model for the next round\n",
        "    global_model = best_client_model\n",
        "\n",
        "    # Update all clients with the global model\n",
        "    for index, client in enumerate(clients):\n",
        "        client.primary_model = global_model\n",
        "\n",
        "    # Evaluate the global model on the new test data\n",
        "    global_accuracy = get_accuracy(global_model, test_sequences, test_labels)\n",
        "    global_model_accuracy.append(global_accuracy)\n",
        "\n",
        "    clients_train_accuracies.append(epoch_train_accuracies)\n",
        "    clients_test_accuracies.append(epoch_test_accuracies)\n",
        "\n",
        "    print(f\"Global Model Accuracy in Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    # Save results for the current iteration of the client in the federated round\n",
        "    from google.colab import drive\n",
        "\n",
        "    # Step 1: Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Step 2: Define the save path in Google Drive\n",
        "    save_path = '/content/drive/MyDrive/DQFL_Genome_IID_Global_10Clients_31_03_2025.csv'\n",
        "\n",
        "\n",
        "    # Save accuracies to CSV after each epoch (or at the end of all epochs)\n",
        "    save_accuracies_to_csv(global_model_accuracy, clients_train_accuracies, clients_test_accuracies, filename=save_path)\n",
        "    # After each round, reset callback state to prepare for the next round\n",
        "    reset_callback_graph()\n",
        "    print(f\"File saved to {save_path}\")\n",
        "\n",
        "#print(\"Accuracy data saved to\", csv_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to save global loss\n",
        "global_loss_csv = '/content/drive/My Drive/Federated_Global_Loss_29_01_2025.csv'\n",
        "\n",
        "# Write headers to the CSV file (only at the beginning)\n",
        "if epoch == 0:\n",
        "    with open(global_loss_csv, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Federated Round\", \"Global Loss\"])\n",
        "\n",
        "# Append the global loss after each round\n",
        "with open(global_loss_csv, mode='a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([epoch, global_loss])\n"
      ],
      "metadata": {
        "id": "8RwQoPVIRKbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot global loss over federated rounds\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(global_loss_per_round)), global_loss_per_round, marker='o', color='blue', label=\"Global Loss\")\n",
        "plt.xlabel(\"Federated Round\")\n",
        "plt.ylabel(\"Global Loss\")\n",
        "plt.title(\"Global Loss Over Federated Rounds\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4D9XnmHoRPez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zs_rlJT5XJv"
      },
      "source": [
        "Split data as iid and non-iid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pgm1g3VHfXC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#Introduce custome cross entropy function\n",
        "import numpy as np\n",
        "\n",
        "# Callback for updating learning rate dynamically with deep unfolding principles\n",
        "def deep_unfolding_learning_rate_adjustment(obj_func_eval, gradients=None, client_id=None, layer=None):\n",
        "    global gradient_moving_avg, learning_rates, perturbations,meta_alpha, meta_epsilon, momentum\n",
        "\n",
        "    # Initialize moving average for gradients\n",
        "    if gradients is not None:\n",
        "        if not isinstance(gradient_moving_avg, np.ndarray) or gradient_moving_avg.size == 0:\n",
        "            gradient_moving_avg = gradients\n",
        "        else:\n",
        "            # Update moving average of gradients (Momentum)\n",
        "            gradient_moving_avg = momentum * gradient_moving_avg + (1 - momentum) * gradients\n",
        "        # Calculate the average gradient\n",
        "        avg_gradient = np.mean(gradient_moving_avg)\n",
        "\n",
        "        # Normalize delta_lr by L2 norm of the gradient\n",
        "        norm_gradient = np.linalg.norm(gradients)\n",
        "\n",
        "        '''\n",
        "        # Normalization to prevent instability\n",
        "        norm_gradient = gradients / (np.linalg.norm(gradients) + 1e-8)\n",
        "        avg_gradient = np.mean(norm_gradient)\n",
        "        '''\n",
        "        # Trainable scaling for deep unfolding (meta-parameter)\n",
        "        meta_alpha = 0.01  # This can be learned via a hypernetwork or meta-learning\n",
        "        meta_epsilon = 1e-6  # Small offset to ensure numerical stability\n",
        "        # Gradually adjust learning rate based on gradient signs and magnitude\n",
        "        # This formula gradually adds or subtracts from the learning rate instead of multiplication\n",
        "        delta_lr = meta_alpha * np.sign(avg_gradient) * np.sqrt(np.abs(avg_gradient) + meta_epsilon) / (norm_gradient + 1e-6)\n",
        "\n",
        "        #delta_lr = meta_alpha * np.sign(avg_gradient) * np.sqrt(np.abs(avg_gradient) + meta_epsilon)\n",
        "    # Apply gradual adjustment (either addition or subtraction based on the direction of the gradient)\n",
        "        if avg_gradient > 0:\n",
        "            delta_lr = delta_lr - 0.001  # Decrease if gradient is positive (potential overfitting)\n",
        "        else:\n",
        "            delta_lr = delta_lr + 0.001  # Increase if gradient is negative (potential underfitting)\n",
        "    else:\n",
        "        delta_lr = 0\n",
        "\n",
        "    # Compute new learning rate with clamping for stability\n",
        "    new_lr = max(0.001, min(5.0, learning_rates[-1] + delta_lr)) if learning_rates else initial_learning_rate\n",
        "\n",
        "    # Update per-client, per-layer information if federated\n",
        "    if client_id is not None and layer is not None:\n",
        "        if client_id not in client_data:\n",
        "            client_data[client_id] = {'federated_layers': {}}\n",
        "        if layer not in client_data[client_id]['federated_layers']:\n",
        "            client_data[client_id]['federated_layers'][layer] = {'objective_values': [], 'learning_rates': []}\n",
        "\n",
        "        # Store loss and learning rate for the specific client and layer\n",
        "        client_data[client_id]['federated_layers'][layer]['objective_values'].append(obj_func_eval)\n",
        "        client_data[client_id]['federated_layers'][layer]['learning_rates'].append(new_lr)\n",
        "\n",
        "    # Store global metrics\n",
        "    objective_func_vals.append(obj_func_eval)  # Store the loss value globally\n",
        "    learning_rates.append(new_lr)  # Append the new learning rate to the history\n",
        "\n",
        "    # Update meta-parameters (meta_alpha and meta_epsilon) using gradient descent\n",
        "    #meta_gradients = compute_meta_gradients(gradients, avg_gradient, delta_lr)\n",
        "    #meta_alpha -= meta_learning_rate * meta_gradients['alpha']\n",
        "    #meta_epsilon -= meta_learning_rate * meta_gradients['epsilon']\n",
        "\n",
        "    # Debug output for analysis\n",
        "    # print(f\"Objective Function Value: {obj_func_eval:.6f}, New Learning Rate: {new_lr:.6f}\")\n",
        "\n",
        "    return new_lr\n",
        "\n",
        "\n",
        "def callback_graph(weights, loss):\n",
        "    \"\"\"Callback to log and synchronize loss during training.\"\"\"\n",
        "    #print(f\"Loss = {loss}\")\n",
        "    if len(objective_func_vals) == 0 or loss != objective_func_vals[-1]:\n",
        "        objective_func_vals.append(loss)\n",
        "\n",
        "spsa_optimizer = SPSA(maxiter=50, learning_rate=0.01, perturbation = 0.15, callback=lambda nfev, params, obj_func_eval, stepsize, accept: deep_unfolding_learning_rate_adjustment(obj_func_eval, stepsize))\n",
        "\n",
        "\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file_path = 'federated_learning_accuracy.csv'\n",
        "\n",
        "# Open the CSV file in write mode and add headers (if starting fresh)\n",
        "with open(csv_file_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Write the header\n",
        "    writer.writerow(['Epoch', 'Global Accuracy'] + [f'Client {i+1} Final Accuracy' for i in range(num_clients)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a_q7UqBTcju"
      },
      "source": [
        "new ways to average"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1wzVu0QDWaGORXG5Ye_xJj4xO8ALG8Hud",
      "authorship_tag": "ABX9TyPObkAgbme7vgy4YgOK6KW4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}