{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fdPhLlT9q3Np"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Us3DV6Sfq6Mw","executionInfo":{"status":"ok","timestamp":1738109368829,"user_tz":-660,"elapsed":21891,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"outputs":[],"source":["\n","%%capture\n","!pip install genomic-benchmarks\n","!pip install qiskit qiskit_machine_learning qiskit_algorithms\n","!pip install qiskit-aer\n","!pip install qiskit\n","!pip install qiskit_machine_learning"]},{"cell_type":"code","source":["%%capture\n","!pip install qiskit-aer-gpu"],"metadata":{"id":"7mzh_uuVQF6u","executionInfo":{"status":"ok","timestamp":1738109395748,"user_tz":-660,"elapsed":11751,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Check qiskit version"],"metadata":{"id":"xsIMWIL4fWxC"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"er-_TE10rI2c","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1738109396334,"user_tz":-660,"elapsed":589,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}},"outputId":"825184bc-c8c6-4294-d9d3-d953cad96b61"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.3.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["import qiskit\n","qiskit.__version__"]},{"cell_type":"markdown","source":["Check qiskit"],"metadata":{"id":"YL9eHyrnfZeW"}},{"cell_type":"code","source":["!pip show qiskit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2X8aux3fUp1","executionInfo":{"status":"ok","timestamp":1738109397879,"user_tz":-660,"elapsed":1553,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}},"outputId":"8d66545e-00e3-4057-897e-0ff8014ca735"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: qiskit\n","Version: 1.3.2\n","Summary: An open-source SDK for working with quantum computers at the level of extended quantum circuits, operators, and primitives.\n","Home-page: https://www.ibm.com/quantum/qiskit\n","Author: \n","Author-email: Qiskit Development Team <qiskit@us.ibm.com>\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.11/dist-packages\n","Requires: dill, numpy, python-dateutil, rustworkx, scipy, stevedore, symengine, sympy, typing-extensions\n","Required-by: qiskit-aer, qiskit-aer-gpu, qiskit-algorithms, qiskit-machine-learning\n"]}]},{"cell_type":"code","source":["%%capture\n","!pip install qiskit-ibm-provider"],"metadata":{"id":"x6FJEmAtfgCM","executionInfo":{"status":"ok","timestamp":1738109406088,"user_tz":-660,"elapsed":8218,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install qiskit-ibm-runtime"],"metadata":{"id":"YBjvGwrKQQNH","executionInfo":{"status":"ok","timestamp":1738109420488,"user_tz":-660,"elapsed":14408,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from qiskit import transpile\n","from qiskit_aer import AerSimulator\n","\n","sim = AerSimulator()"],"metadata":{"id":"7Ckrp081QLbG","executionInfo":{"status":"ok","timestamp":1738109421455,"user_tz":-660,"elapsed":970,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from qiskit_ibm_runtime import QiskitRuntimeService\n","provider = QiskitRuntimeService(channel='ibm_quantum', token=\"8794f4d4e869af0d7b0e7ed41d1d02b36d728c7ba291e4717cfb4bc18cb885935c13441aacba40358251af1e2547e16420b3d522aad782b503b82fc66b4717e5\")\n","\n","backend = provider.backend(\"ibm_sherbrooke\")\n","\n","print(\n","    f\"Name: {backend.name}\\n\"\n","    f\"Version: {backend.version}\\n\"\n","    f\"No. of qubits: {backend.num_qubits}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wYrkZfULQpDQ","executionInfo":{"status":"ok","timestamp":1738109433308,"user_tz":-660,"elapsed":11856,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}},"outputId":"755be312-9f21-42f2-b719-b2c28581e5ab"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: ibm_sherbrooke\n","Version: 2\n","No. of qubits: 127\n","\n"]}]},{"cell_type":"code","source":["print(backend.status())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZIKYWhkiSUv","executionInfo":{"status":"ok","timestamp":1738109433308,"user_tz":-660,"elapsed":5,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}},"outputId":"86f36588-f042-4cd8-b4ef-d5c532aabe4b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["<qiskit_ibm_runtime.models.backend_status.BackendStatus object at 0x7ccd36c84590>\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"id":"m5M_eDObB6OI","executionInfo":{"status":"ok","timestamp":1738109433308,"user_tz":-660,"elapsed":4,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"outputs":[],"source":["#-------Split data for federated Setting--------#\n","num_epochs = 10 #5\n","max_train_iterations = 100\n","samples_per_epoch=50\n","#backend = Aer.get_backend('aer_simulator')\n","word_size = 40\n","\n","# Configuration variables\n","num_clients = 3\n","num_federated_layers = 10\n","num_deep_unfolding_iterations = 5\n","initial_learning_rate = 0.15\n","meta_learning_rate=1e-4\n","initial_perturbation = 0.15\n","momentum = 0.95\n","gradient_moving_avg = 0\n","\n","# Define federated learning with accuracy tracking\n","num_features = 5\n","global_model_weights, global_model_accuracy = {}, []\n","clients_train_accuracies, clients_test_accuracies = [], []\n","\n","# Define the federated learning parameters\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51494,"status":"ok","timestamp":1738109484799,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"},"user_tz":-660},"id":"LpAw5S3imZQW","outputId":"ffbbfbbb-d458-4211-8c60-3bd0ccfe1c1d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/genomic_benchmarks/utils/datasets.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1JW0-eTB-rJXvFcglqBo3pFZi1kyIWC3X\n","From (redirected): https://drive.google.com/uc?id=1JW0-eTB-rJXvFcglqBo3pFZi1kyIWC3X&confirm=t&uuid=d626b0eb-90e0-4938-86b5-8f388fd7d87e\n","To: /root/.genomic_benchmarks/demo_human_or_worm.zip\n","100%|██████████| 28.9M/28.9M [00:00<00:00, 184MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Number of samples in the test set: 25000\n","Number of samples in the train set: 75000\n","First sample int the data_set variable: \n","('CACTCACAGTACCTGGTTTTAACTTTGTATCACTGAAAGAGGCACTGAAGAAGTGCTCTTTCATGGCTGGTTAGCAGTCAGTCCTCAATTGCCTACACCATCTCTCCCCAGTCCCTTACTGGGGATGTACAGCATAGAGACATAATCTGTGCACTTGGGGGACAGTGTAGTGACTAGGAGACTTTATATTGATCTCAGTG', 0)\n","\n","First 5 samples in the word_combinations dict.\n","CACTCACAGTACCTGGTTTTAACTTTGTATCACTGAAAGA 1\n","ACTCACAGTACCTGGTTTTAACTTTGTATCACTGAAAGAG 2\n","CTCACAGTACCTGGTTTTAACTTTGTATCACTGAAAGAGG 3\n","TCACAGTACCTGGTTTTAACTTTGTATCACTGAAAGAGGC 4\n","CACAGTACCTGGTTTTAACTTTGTATCACTGAAAGAGGCA 5\n","First 5 samples of encoded data:\n","First 5 samples of encoded shuffled data:\n","First 5 samples of scaled encoded shuffled data:\n","Length of np_train_data: 3000\n","Length of np_test_data: 500\n","Client 0 Test Data Length: 166\n","Client 1 Test Data Length: 166\n","Client 2 Test Data Length: 166\n"]}],"source":["\n","from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n","import time\n","from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n","from qiskit_algorithms.optimizers import COBYLA\n","from qiskit_machine_learning.algorithms.classifiers import VQC\n","from qiskit.primitives import BackendSampler\n","from functools import partial\n","from qiskit_aer import Aer\n","\n","from qiskit_machine_learning.neural_networks import SamplerQNN\n","from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n","from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n","from qiskit.primitives import BackendSampler\n","from qiskit_algorithms.optimizers import SPSA\n","import numpy as np\n","import time\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","from qiskit_algorithms.utils import algorithm_globals # Import algorithm_globals\n","\n","# Set random seed for reproducibility using algorithm_globals\n","algorithm_globals.random_seed = 42  # Set seed globally\n","\n","\n","test_set = DemoHumanOrWorm(split='test', version=0)\n","train_set = DemoHumanOrWorm(split='train', version=0)\n","\n","data_set = train_set\n","# data_set = train_set + test_set\n","len(data_set)\n","\n","\n","print(f\"Number of samples in the test set: {len(test_set)}\")\n","print(f\"Number of samples in the train set: {len(train_set)}\")\n","\n","from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n","\n","test_set = DemoHumanOrWorm(split='test', version=0)\n","train_set = DemoHumanOrWorm(split='train', version=0)\n","\n","data_set = train_set\n","# data_set = train_set + test_set\n","len(data_set)\n","\n","from collections import defaultdict\n","import numpy as np\n","\n","\n","word_combinations = defaultdict(int)\n","iteration = 1\n","for text, _ in data_set:\n","    for i in range(len(text)):\n","        word = text[i:i+word_size]\n","        if word_combinations.get(word) is None:\n","          word_combinations[word] = iteration\n","          iteration += 1\n","\n","\n","\n","print(\"First sample int the data_set variable: \")\n","print(data_set[0])\n","\n","print(\"\\nFirst 5 samples in the word_combinations dict.\")\n","for key, value in list(word_combinations.items())[:5]:\n","    print(key, value)\n","\n","\n","import numpy as np\n","# Preprocess the training set\n","np_data_set = []\n","for i in range(len(data_set)):\n","    sequence, label = data_set[i]\n","    sequence = sequence.strip()  # Remove any leading/trailing whitespace\n","    words = [sequence[i:i + word_size] for i in range(0, len(sequence), word_size)]  # Split the sequence into 4-letter words\n","    int_sequence = np.array([word_combinations[word] for word in words])\n","    data_point = {'sequence': int_sequence, 'label': label}\n","    np_data_set.append(data_point)\n","\n","\n","print(\"First 5 samples of encoded data:\")\n","np_data_set[:5]\n","\n","\n","np.random.shuffle(np_data_set)\n","print(\"First 5 samples of encoded shuffled data:\")\n","np_data_set[:5]\n","from sklearn.preprocessing import MinMaxScaler\n","\n","sequences = np.array([item['sequence'] for item in np_data_set])\n","sequences = np.vstack(sequences)\n","\n","scaler = MinMaxScaler()\n","\n","sequences_scaled = scaler.fit_transform(sequences)\n","\n","for i, item in enumerate(np_data_set):\n","    item['sequence'] = sequences_scaled[i]\n","\n","print(\"First 5 samples of scaled encoded shuffled data:\")\n","np_data_set[:5]\n","\n","\n","np_train_data = np_data_set[:3000] #15000\n","np_test_data = np_data_set[-500:]#5000\n","\n","print(f\"Length of np_train_data: {len(np_train_data)}\")\n","print(f\"Length of np_test_data: {len(np_test_data)}\")\n","\n","test_sequences = [data_point[\"sequence\"] for data_point in np_test_data]\n","test_labels = [data_point[\"label\"] for data_point in np_test_data]\n","test_sequences = np.array(test_sequences)\n","test_labels = np.array(test_labels)\n","\n","\n","#---------------------------------------\n","\n","\n","class Client:\n","   def __init__(self, data, test_data):  # Add test_data to __init__\n","        self.data = data\n","        self.test_data = test_data  # Store test_data as an attribute\n","        self.models = []\n","        self.train_scores = []\n","        self.test_scores = []\n","        self.primary_model = None\n","\n","def split_dataset(num_clients, num_epochs, samples_per_epoch):\n","    clients = []\n","    # Split test data across clients\n","    test_samples_per_client = len(np_test_data) // num_clients\n","\n","    for i in range(num_clients):\n","        client_data = []\n","        for j in range(num_epochs):\n","            start_idx = (i * num_epochs * samples_per_epoch) + (j * samples_per_epoch)\n","            end_idx = (i * num_epochs * samples_per_epoch) + ((j + 1) * samples_per_epoch)\n","            client_data.append(np_train_data[start_idx:end_idx])\n","\n","        # Assign a subset of the test data to each client\n","        test_start_idx = i * test_samples_per_client\n","        test_end_idx = (i + 1) * test_samples_per_client\n","        client_test_data = np_test_data[test_start_idx:test_end_idx]\n","\n","        # Create Client instance with both train and test data\n","        clients.append(Client(client_data, client_test_data))\n","\n","    return clients\n","\n","clients = split_dataset(num_clients, num_epochs, samples_per_epoch)\n","\n","# Verify test data distribution across clients\n","for index, client in enumerate(clients):\n","    print(f\"Client {index} Test Data Length: {len(client.test_data)}\")\n"]},{"cell_type":"markdown","metadata":{"id":"LIg_mUrp3_2J"},"source":["Data Load and preprocessing"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"3OTrftC6uZd_","executionInfo":{"status":"ok","timestamp":1738109484800,"user_tz":-660,"elapsed":11,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"outputs":[],"source":["def split_dataset_for_epochs(num_clients, num_epochs, train_data, test_data, samples_per_epoch):\n","    \"\"\"\n","    Split the dataset across multiple epochs and clients.\n","\n","    Args:\n","        num_clients (int): Number of clients.\n","        num_epochs (int): Number of epochs.\n","        train_data (list): List of training data points.\n","        test_data (list): List of test data points.\n","        samples_per_epoch (int): Number of samples per epoch.\n","\n","    Returns:\n","        list: A list of Client objects with assigned data for each epoch.\n","    \"\"\"\n","    clients = []\n","\n","    # Split the training data across epochs and clients\n","    train_samples_per_client = len(train_data) // num_clients\n","\n","    for i in range(num_clients):\n","        client_data_for_epochs = []\n","\n","        for epoch in range(num_epochs):\n","            start_idx = (i * num_epochs * samples_per_epoch) + (epoch * samples_per_epoch)\n","            end_idx = (i * num_epochs * samples_per_epoch) + ((epoch + 1) * samples_per_epoch)\n","            client_data_for_epochs.append(train_data[start_idx:end_idx])\n","\n","        # Assign test data to each client\n","        test_samples_per_client = len(test_data) // num_clients\n","        test_start_idx = i * test_samples_per_client\n","        test_end_idx = (i + 1) * test_samples_per_client\n","        client_test_data = test_data[test_start_idx:test_end_idx]\n","\n","        # Create a Client instance with epoch-specific data\n","        clients.append(Client(client_data_for_epochs, client_test_data))\n","\n","    return clients\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"vHHwtTUF65Gp","executionInfo":{"status":"ok","timestamp":1738109484800,"user_tz":-660,"elapsed":10,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"outputs":[],"source":["\n","import csv\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","from qiskit_machine_learning.neural_networks import SamplerQNN\n","from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n","from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n","from qiskit.primitives import BackendSampler\n","from qiskit_algorithms.optimizers import SPSA\n","from qiskit_algorithms.utils import algorithm_globals\n","from qiskit_algorithms.optimizers import COBYLA\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.metrics import log_loss\n","from tqdm import tqdm\n","\n","\n","# Callback function to capture the loss values\n","objective_func_vals = []  # Global list to store loss values\n","learning_rates = []\n","perturbations = []\n","# Data structure for tracking per-client, per-layer objective function values\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"o0CaYcz9FQw7","executionInfo":{"status":"ok","timestamp":1738109484800,"user_tz":-660,"elapsed":10,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"outputs":[],"source":["\n","\n","import os  # For handling directories\n","\n","# Define the directory to save the plots\n","output_dir = \"federated_round_plots\"\n","os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n","# Initialize a global variable to track the round number\n","current_round = 1\n","\n","# Callback for visualization, gradient smoothing, and learning rate adjustment in deep unfolding\n","def deep_unfolding_learning_rate_adjustment(parameters, obj_func_eval, gradients=None,round_number=0):\n","    global gradient_moving_avg, learning_rates, perturbations,current_round\n","\n","    clear_output(wait=True)\n","\n","    # Save the objective function value for visualization\n","    objective_func_vals.append(obj_func_eval)\n","\n","    # If gradients are provided, smooth the gradient using momentum\n","    if gradients is not None:\n","        gradient_moving_avg = momentum * gradient_moving_avg + (1 - momentum) * gradients  # Apply moving average\n","        delta_lr = 0.05 * gradient_moving_avg  # Adjust learning rate based on the smoothed gradient\n","        delta_perturbation = 0.1 * gradient_moving_avg  # Adjust perturbation based on the same gradient\n","    else:\n","        delta_lr = 0  # No gradient info available in this iteration\n","        delta_perturbation = 0\n","\n","    # Update learning rate and perturbation\n","    if len(learning_rates) > 0:\n","        new_lr = max(0.001, learning_rates[-1] + delta_lr)  # Ensure learning rate is positive and non-zero\n","        new_perturbation = max(0.001, perturbations[-1] + delta_perturbation)  # Ensure perturbation is positive\n","    else:\n","        new_lr = initial_learning_rate\n","        new_perturbation = initial_perturbation\n","\n","    learning_rates.append(new_lr)\n","    perturbations.append(new_perturbation)\n","\n","    import matplotlib.pyplot as plt\n","\n","    # Visualization of learning rate and perturbation\n","    plt.figure(figsize=(10, 12))  # Adjust figure size for better spacing\n","\n","    # Plot Objective Function Value\n","    plt.subplot(3, 1, 1)\n","    plt.plot(range(len(objective_func_vals)), objective_func_vals, label=\"Objective Function Value\", color='blue')\n","    plt.xlabel(\"Iteration\")\n","    plt.ylabel(\"Objective Function Value\")\n","    plt.title(\"Objective Function Over Iterations\")\n","    plt.legend(loc=\"best\")\n","    plt.grid(True)  # Add grid for better readability\n","\n","    # Plot Learning Rate\n","    plt.subplot(3, 1, 2)\n","    plt.plot(range(len(learning_rates)), learning_rates, label=\"Learning Rate\", color='green')\n","    plt.xlabel(\"Iteration\")\n","    plt.ylabel(\"Learning Rate\")\n","    plt.title(\"Learning Rate Over Iterations\")\n","    plt.legend(loc=\"best\")\n","    plt.grid(True)\n","\n","    # Plot Perturbation\n","    plt.subplot(3, 1, 3)\n","    plt.plot(range(len(perturbations)), perturbations, label=\"Perturbation\", color='red')\n","    plt.xlabel(\"Iteration\")\n","    plt.ylabel(\"Perturbation\")\n","    plt.title(\"Perturbation Over Iterations\")\n","    plt.legend(loc=\"best\")\n","    plt.grid(True)\n","\n","    plt.tight_layout(pad=3.0)  # Add padding between subplots\n","    # Save the plot after each federated round\n","    #plot_filename = os.path.join(output_dir, f\"federated_round_{current_round}.png\")\n","    #plt.savefig(plot_filename)  # Save the figure\n","    #plt.show()\n","    plt.close()  # Close the plot to free memory\n","\n","    # Increment the round number for the next call\n","    current_round += 1\n","\n","\n","# Define the SPSA callback to capture gradients and update learning rate and perturbation dynamically\n","def spsa_callback(nfev, parameters, obj_func_eval, stepsize, accept):\n","    # Assuming `stepsize` contains gradient information or its approximation\n","    gradients = stepsize\n","    deep_unfolding_learning_rate_adjustment(parameters, obj_func_eval, gradients)\n","\n","# Custom SPSA optimizer with learnable learning rate and perturbation\n","class LearnableLRPerturbationSPSA(SPSA):\n","    def __init__(self, initial_lr=1e-4, initial_perturbation=0.01, lr_alpha=0.1, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.lr = initial_lr  # Initial learning rate\n","        self.perturbation = initial_perturbation  # Initial perturbation\n","        self.lr_alpha = lr_alpha  # Learning rate and perturbation update speed\n","\n","    def _update_learning_rate_and_perturbation(self, gradient, obj_func_eval):\n","        \"\"\"\n","        Update both learning rate and perturbation based on gradient and objective function evaluation.\n","        The learning rate increases if the objective function improves and decreases otherwise.\n","        \"\"\"\n","        # Use the gradient sign to determine if we should increase or decrease\n","        grad_lr = np.sign(np.mean(gradient))  # Average gradient sign across parameters\n","\n","        if grad_lr > 0:  # Objective function is improving\n","            self.lr += self.lr_alpha * abs(grad_lr)  # Increase learning rate\n","            self.perturbation += self.lr_alpha * abs(grad_lr)  # Increase perturbation\n","        else:  # Objective function is getting worse\n","            self.lr -= self.lr_alpha * abs(grad_lr)  # Decrease learning rate\n","            self.perturbation -= self.lr_alpha * abs(grad_lr)  # Decrease perturbation\n","\n","        # Ensure both learning rate and perturbation are positive\n","        self.lr = max(0.001, self.lr)\n","        self.perturbation = max(0.001, self.perturbation)\n","\n","    def step(self, gradient, obj_func_eval):\n","        \"\"\"\n","        Perform optimization step for both parameters, learning rate, and perturbation.\n","        Use the objective function evaluation to dynamically adjust learning rate and perturbation.\n","        \"\"\"\n","        self._update_learning_rate_and_perturbation(gradient, obj_func_eval)\n","        return super().step(gradient)  # Perform SPSA step for parameters\n","\n","    def reset(self):\n","        \"\"\"\n","        Reset the optimizer state (learning rates, perturbations, and gradient moving averages) for the next round.\n","        \"\"\"\n","        self.lr = initial_learning_rate\n","        self.perturbation = initial_perturbation\n","        self.gradient_moving_avg = 0  # Reset the moving average of the gradient\n","        learning_rates.clear()  # Reset the learning rates history\n","        perturbations.clear()  # Reset the perturbations history\n","        objective_func_vals.clear()  # Clear the objective function history"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"X46XXHW1s4tR","executionInfo":{"status":"ok","timestamp":1738109484800,"user_tz":-660,"elapsed":9,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"outputs":[],"source":["# Create optimizer with learnable learning rate and perturbation\n","spsa_optimizer = LearnableLRPerturbationSPSA(\n","      maxiter=50, learning_rate=initial_learning_rate, perturbation=initial_perturbation, callback=spsa_callback, lr_alpha=0.01\n",")"]},{"cell_type":"code","source":["from qiskit.compiler import transpile  # Import transpile"],"metadata":{"id":"QZH5IdnOioue","executionInfo":{"status":"ok","timestamp":1738109499278,"user_tz":-660,"elapsed":8,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"-3Rhf0Ft7CI2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738109499277,"user_tz":-660,"elapsed":14486,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}},"outputId":"c4eeee08-56ea-466e-907b-d1cae3fae553"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-72da2af05627>:11: DeprecationWarning: The class ``qiskit.primitives.backend_sampler.BackendSampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `BackendSampler` class is `BackendSamplerV2`.\n","  sampler = BackendSampler(backend=simulator)\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#from qiskit.primitives import BackendSampler\n","#sampler = BackendSampler(backend=backend)\n","\n","from qiskit_aer import AerSimulator\n","from qiskit.primitives import BackendSampler\n","\n","# Initialize the AerSimulator\n","simulator = AerSimulator()\n","\n","# Use BackendSamplerV2 instead of BackendSampler\n","sampler = BackendSampler(backend=simulator)\n","\n","# Transpile circuits for AerSimulator\n","#transpiled_circuit = transpile(qc, simulator, optimization_level=3)\n","#======================================================\n","# Initialize QNN model\n","def initialize_model(num_features,initial_params,sampler):\n","    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)\n","    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n","    qc = feature_map.compose(ansatz)\n","\n","    # Pass the original backend (not sampler) to transpile\n","    #transpiled_circuit = transpile(qc, sampler.backend)\n","    # Transpile circuits for AerSimulator\n","    transpiled_circuit = transpile(qc, simulator, optimization_level=3)\n","\n","    # Create optimizer with learnable learning rate and perturbation\n","    spsa_optimizer = LearnableLRPerturbationSPSA(\n","      maxiter=50, learning_rate=initial_learning_rate, perturbation=initial_perturbation, callback=spsa_callback, lr_alpha=0.01\n",")\n","    def parity(x):\n","        return \"{:b}\".format(x).count(\"1\") % 2\n","\n","    sampler_qnn = SamplerQNN(\n","        circuit=transpiled_circuit,\n","        interpret=parity,\n","        output_shape=2,\n","        input_params=feature_map.parameters,\n","        weight_params=ansatz.parameters,\n","        sampler=sampler\n","    )\n","\n","\n","    # Define the neural network classifier\n","    qnn_classifier = NeuralNetworkClassifier(\n","      neural_network=sampler_qnn,\n","      optimizer=spsa_optimizer,\n","      loss='squared_error',\n","      initial_point=initial_params,  # Initialize with the starting parameters\n",")\n","\n","\n","    return qnn_classifier\n","\n","#=====================================================\n","from google.colab import drive\n","import csv\n","# Step 1: Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Step 2: Define the save path in Google Drive\n","csv_file = '/content/drive/My Drive/Dqfl_best_avg_local_aer_sim_LargeData_withloss.csv'\n","\n","# Step 3: Define headers for the CSV\n","headers = [\"Federated Round\", \"Client Number\", \"Iteration\", \"Objective Function Value\",\n","           \"Training Accuracy\", \"Test Accuracy\", \"Learning Rate\", \"Perturbation\"]\n","\n","# Open the CSV file and write headers if it's the first time writing to the file\n","with open(csv_file, mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(headers)\n","\n","# Example of saving results for each federated round and client\n","def save_results(federated_round, client_id, iteration, obj_func_val, train_acc, test_acc, learning_rate, perturbation):\n","    with open(csv_file, mode='a', newline='') as file:  # Open file in append mode\n","        writer = csv.writer(file)\n","        writer.writerow([federated_round, client_id, iteration, obj_func_val, train_acc, test_acc, learning_rate, perturbation])\n","#=====================================================\n","\n","#from qiskit.primitives import BackendSampler\n","#sampler = BackendSampler(backend=backend)\n","\n","# Federated learning loop per client\n","def train_qnn_model(client_data, client_test_data, model=None, client_id=None, layer=None):\n","\n","    global learning_rates, perturbations, objective_func_vals\n","    print(\"Client Data Structure:\")  # Add this line to print the structure\n","    print(client_data)                # This line prints the actual data\n","    print(type(client_data))           # This line prints the data type\n","    num_features = client_data[0][\"sequence\"].shape[0]\n","\n","    #initial_params = np.random.rand(RealAmplitudes(client_data.shape[1], reps=4).num_parameters)  # Initialize params\n","    initial_params = np.random.rand(RealAmplitudes(len(client_data[0][\"sequence\"]), reps=3).num_parameters)\n","\n","    if model is None:\n","        model = initialize_model(num_features, initial_params,sampler)\n","\n","    train_sequences = np.array([data_point[\"sequence\"] for data_point in client_data])\n","    train_labels = np.array([data_point[\"label\"] for data_point in client_data])\n","    test_sequences = np.array([data_point[\"sequence\"] for data_point in client_test_data])\n","    test_labels = np.array([data_point[\"label\"] for data_point in client_test_data])\n","\n","    train_accuracies, test_accuracies, total_time = [], [], 0\n","\n","    train_accuracies = []\n","    test_accuracies = []\n","\n","    # Deep Unfolding with multiple iterations\n","    # Continue training with learned weights and adjust learning rate based on performance and gradients.\n","    total_time = 0\n","    current_params = initial_params  # Start with the initial parameters\n","\n","    for i in range(num_deep_unfolding_iterations):\n","        print(\"\\n\")\n","        print(f\"Deep Unfolding Iteration {i+1}/{num_deep_unfolding_iterations}\")\n","        start_time = time.time()\n","        model.fit(train_sequences, train_labels)\n","        end_time = time.time()\n","        total_time += end_time - start_time\n","\n","        # After training, retrieve the updated parameters from the optimizer\n","        current_params = model.weights\n","        print(f\"Trained parameters after iteration {i+1}: {current_params}\")\n","\n","        # Store final weights and learning rate for next round\n","        final_learning_rate = learning_rates[-1]\n","        final_perturbation = perturbations[-1]\n","\n","        # Evaluate the model performance\n","        train_accuracy = model.score(train_sequences, train_labels)\n","        test_accuracy = model.score(test_sequences, test_labels)\n","\n","        # Store accuracies for future reference\n","        train_accuracies.append(train_accuracy)\n","        test_accuracies.append(test_accuracy)\n","\n","\n","        # Write the results to the CSV file\n","        save_results(layer, client_id, i+1, objective_func_vals[-1], train_accuracy * 100, test_accuracy * 100, final_learning_rate, final_perturbation)\n","\n","        #with open(csv_file, mode='a', newline='') as file:\n","          #writer = csv.writer(file)\n","         #writer.writerow([i+1, objective_func_vals[-1], train_accuracy * 100, test_accuracy * 100, final_learning_rate, final_perturbation])\n","\n","        # Update the learning rate for the next iteration based on gradients from SPSA\n","        spsa_optimizer.learning_rate = learning_rates[-1]\n","        model.initial_point = current_params\n","\n","        # Log performance\n","        print(f\"Iteration {i+1} - Learning Rate: {final_learning_rate:.6f}\")\n","        print(f\"Iteration {i+1} - Training Accuracy: {train_accuracy * 100:.2f}%\")\n","        print(f\"Iteration {i+1} - Test Accuracy: {test_accuracy * 100:.2f}%\")\n","\n","    return model, train_accuracy, train_accuracy, total_time\n","\n","\n"]},{"cell_type":"code","source":["# Step to empty the CSV file before starting a new run\n","def clear_csv_file():\n","    \"\"\"\n","    Clears the CSV file by overwriting it with headers or leaving it blank.\n","    \"\"\"\n","    with open(csv_file, mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","        # Uncomment the next line to write headers for the new run\n","        writer.writerow(headers)\n","        # Leave it blank if you prefer not to include headers\n","        # pass\n","\n"],"metadata":{"id":"AkLnHZoPTwB6","executionInfo":{"status":"ok","timestamp":1738109499278,"user_tz":-660,"elapsed":7,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"id":"zpK0oXUHzPtm","executionInfo":{"status":"ok","timestamp":1738109499278,"user_tz":-660,"elapsed":7,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"outputs":[],"source":["def get_accuracy(model, test_sequences, test_labels):\n","    \"\"\"\n","    Evaluate the accuracy of the given model on the test dataset.\n","\n","    Parameters:\n","        model: The trained model to evaluate.\n","        num_features: The number of features in each data sample.\n","        test_sequences: A list or array of test input data (features).\n","        test_labels: A list or array of true labels corresponding to the test data.\n","\n","    Returns:\n","        float: The accuracy of the model as a percentage.\n","    \"\"\"\n","    test_accuracy = model.score(test_sequences, test_labels)\n","    return test_accuracy\n","\n","# Function to extract numerical values of parameters\n","def extract_param_values(model):\n","    #param_values = []\n","    # Loop through each parameter in the circuit and get its bound value\n","    # Retrieve the circuit from the neural network\n","    circuit = model.neural_network.circuit\n","\n","    # Extract the parameter values bound to the circuit\n","    # Use enumerate to get both index and parameter\n","    param_values = {param: circuit.parameters[i] for i, param in enumerate(circuit.parameters)}\n","    return param_values\n","#def set_param_values(model, param_values):\n","    # Retrieve the circuit from the neural network\n","    #circuit = model.neural_network.circuit\n","\n","    # Use assign_parameters to update the parameter values\n","    #circuit.assign_parameters(param_values, inplace=True)\n","# Function to set numerical values of parameters back into the circuit\n","def set_param_values(model, param_values):\n","    # Assign the averaged parameter values back to the circuit\n","    parameter_dict = {param: value for param, value in zip(model.neural_network.circuit.parameters, param_values)}\n","    model.neural_network.circuit.assign_parameters(parameter_dict)\n","\n","\n","# Manually average the numerical values of the parameters across clients\n","def manual_average_weights(epoch_weights):\n","    # Initialize a list to store the summed weights (initialize with zeros)\n","    num_weights = len(epoch_weights[0])  # Number of weights in the model\n","    num_clients = len(epoch_weights)  # Number of clients\n","\n","    # Initialize sum of weights to zero (assuming NumPy array or list of weights)\n","    summed_weights = np.zeros(num_weights)\n","\n","    # Sum the weights from all clients\n","    for client_weights in epoch_weights:\n","        summed_weights += np.array(client_weights)\n","\n","    # Compute the average by dividing the summed weights by the number of clients\n","    average_weights = summed_weights / num_clients\n","\n","    return average_weights\n","\n","def create_model_with_weights(weights):\n","    initial_params = np.random.rand(RealAmplitudes(num_features, reps=1).num_parameters)\n","    model = initialize_model(num_features,weights,sampler)\n","    #set_param_values(model, weights)  # Assign global weights to the model\n","    return model\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"MuhZZtnnzmV5","executionInfo":{"status":"ok","timestamp":1738109499278,"user_tz":-660,"elapsed":6,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"}}},"outputs":[],"source":["\n","# Function to save accuracies to CSV\n","def save_accuracies_to_csv(global_accuracies, clients_train_accuracies, clients_test_accuracies, filename='accuracies.csv'):\n","    with open(filename, mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","\n","        # Write the header row\n","        header = ['Epoch', 'Global Accuracy']\n","        for i in range(len(clients_train_accuracies[0])):  # Assuming all clients have the same number of records\n","            header.append(f'Client {i} Train Accuracy')\n","            header.append(f'Client {i} Test Accuracy')\n","        writer.writerow(header)\n","\n","        # Write the accuracy data for each epoch\n","        for epoch in range(len(global_accuracies)):\n","            row = [epoch, global_accuracies[epoch]]  # Start with epoch and global accuracy\n","            for client_index in range(len(clients_train_accuracies[epoch])):\n","                row.append(clients_train_accuracies[epoch][client_index])  # Add train accuracy for client\n","                row.append(clients_test_accuracies[epoch][client_index])   # Add test accuracy for client\n","            writer.writerow(row)\n"]},{"cell_type":"markdown","metadata":{"id":"tTt4DwZj7Is9"},"source":["Federated Learning Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmYmJR4_7Hux","outputId":"099228ec-c531-4813-f6f5-e240896145ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Trained parameters after iteration 1: [0.32119961 0.27091471 0.19786858 0.83439222 0.15799138 0.80387168\n"," 0.77938304 0.21287114 0.70198683 0.24692411 0.40633959 0.2223929\n"," 0.99507296 0.1988536  0.75240463 1.06850233 0.00392287 0.65285323\n"," 0.49770759 0.27567199]\n","Iteration 1 - Learning Rate: 0.350950\n","Iteration 1 - Training Accuracy: 70.00%\n","Iteration 1 - Test Accuracy: 62.65%\n","\n","\n","Deep Unfolding Iteration 2/5\n"]}],"source":["# Initialize a global list to track global loss\n","global_loss_per_round = []\n","clients = split_dataset_for_epochs(num_clients, num_epochs, np_train_data, np_test_data, samples_per_epoch)\n","\n","# Display information about the data assigned to each client, including epoch-wise splits\n","for idx, client in enumerate(clients):\n","    print(f\"Client {idx + 1}:\")\n","    for epoch in range(num_epochs):\n","        print(f\"  Epoch {epoch + 1}: Train data samples: {len(client.data[epoch])}\")\n","    print(f\"  Test data samples: {len(client.test_data)}\")\n","\n","# Display information about the data assigned to each client\n","#for idx, client in enumerate(clients):\n","    #print(f\"Client {idx + 1}:\")\n","    #print(f\"  Train data samples: {len(client.data)}\")\n","    #print(f\"  Test data samples: {len(client.test_data)}\")\n","\n","    # Accessing the number of features in a sequence\n","    if client.data:\n","        num_features=client.data[0][0]['sequence'].shape[0]  # Access first data point of epoch 0\n","        #num_features = client.data[0]['sequence'].shape[0]\n","        print(f\"  Number of features in a sequence: {num_features}\")\n","\n","def reset_state():\n","    # Reset the objective value, learning rate, and perturbation after each client\n","    global objective_func_vals, learning_rates, perturbations\n","    objective_func_vals = []  # Reset objective values\n","    learning_rates = []  # Reset learning rates\n","    perturbations = []  # Reset perturbations\n","# Function to reset callback graph state after each round\n","def reset_callback_graph():\n","    global gradient_moving_avg, learning_rates, perturbations\n","\n","    # Reset the state variables to start fresh for the next round\n","    gradient_moving_avg = np.zeros_like(gradient_moving_avg)  # Reset gradient moving average\n","    learning_rates = [initial_learning_rate]  # Reset learning rates list to initial value\n","    perturbations = [initial_perturbation]  # Reset perturbations list to initial value\n","import csv\n","\n","# Path to store the best client's data\n","best_client_csv_file = '/content/drive/My Drive/Best_Client_Data_aer_sim_LargeData_withloss.csv'\n","\n","# Write headers to the best client CSV file\n","best_headers = [\"Federated Round\", \"Client Number\"]\n","\n","with open(best_client_csv_file, mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(best_headers)\n","\n","# Function to update the best client data\n","def save_best_client_results(federated_round,best_client_index):\n","    \"\"\"\n","    Save the best client's data to a separate CSV file.\n","    :param best_data: Dictionary containing the best client's data.\n","    \"\"\"\n","    with open(best_client_csv_file, mode='a', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow([federated_round,\n","           best_client_index\n","\n","        ])\n","# Clear the CSV file for a new run\n","clear_csv_file()\n","\n","# Wrap the epoch loop with tqdm\n","for epoch in tqdm(range(num_federated_layers), desc=\"Training Progress\"):\n","    global_model_weights[epoch] = []\n","    epoch_train_accuracies, epoch_test_accuracies = [], []\n","    best_client_index = -1\n","    best_client_accuracy = -1\n","    best_client_model = None\n","    print(\"\\n\")\n","    print(f\"Fed_Epoch: {epoch}\")\n","    round_losses = []  # Track individual client losses for this round\n","\n","    for index, client in enumerate(clients):\n","        print(\"\\n\")\n","        print(f\"Fed_Epoch {epoch}, Client {index + 1}:\")\n","        reset_state()\n","\n","        try:\n","            # Ensure you're using the correct index for data\n","            current_data = client.data[epoch]  # This assumes data is structured in epochs\n","            print(f\"Training data for epoch {epoch}: {len(current_data)}\")\n","        except IndexError:\n","            print(f\"No data available for epoch {epoch} for Client {index + 1}\")\n","            continue  # Skip this client for the current epoch\n","\n","        model, train_score, test_score, train_time = train_qnn_model(\n","            client.data[epoch],\n","            client.test_data,\n","            client_id=index,\n","            layer=epoch,\n","        )\n","\n","        epoch_train_accuracies.append(train_score)\n","        epoch_test_accuracies.append(test_score)\n","\n","        # Check if this client has the best accuracy so far\n","        if test_score > best_client_accuracy:\n","            best_client_accuracy = test_score\n","            best_client_index = index\n","            best_client_model = model  # Directly store the best client's model\n","         # Fetch the client's loss (assumes train_qnn_model returns it)\n","        current_loss = objective_func_vals[-1]  # Fetch latest loss\n","        round_losses.append(current_loss)\n","        # Calculate global loss for the current round as the average of client losses\n","\n","    global_loss = sum(round_losses) / len(round_losses)\n","    global_loss_per_round.append(global_loss)  # Store the global loss\n","\n","    print(f\"Global Loss for Round {epoch}: {global_loss}\")\n","\n","    save_best_client_results(epoch,best_client_index)  # Save to best client CSV\n","    print(f\"Best client for epoch {epoch} is Client {best_client_index + 1} with test accuracy {best_client_accuracy:.2f}\")\n","\n","    # Treat the best client's model as the global model for the next round\n","    global_model = best_client_model\n","\n","    # Update all clients with the global model\n","    for index, client in enumerate(clients):\n","        client.primary_model = global_model\n","\n","    # Evaluate the global model on the new test data\n","    global_accuracy = get_accuracy(global_model, test_sequences, test_labels)\n","    global_model_accuracy.append(global_accuracy)\n","\n","    clients_train_accuracies.append(epoch_train_accuracies)\n","    clients_test_accuracies.append(epoch_test_accuracies)\n","\n","    print(f\"Global Model Accuracy in Epoch {epoch}: {global_accuracy:.2f}\")\n","    print(\"----------------------------------------------------------\")\n","\n","\n","    # Save results for the current iteration of the client in the federated round\n","    from google.colab import drive\n","\n","    # Step 1: Mount Google Drive\n","    drive.mount('/content/drive')\n","\n","    # Step 2: Define the save path in Google Drive\n","    save_path = '/content/drive/MyDrive/DQFL_best_avg_global_aer_sim_LargeData_withloss.csv'\n","\n","\n","    # Save accuracies to CSV after each epoch (or at the end of all epochs)\n","    save_accuracies_to_csv(global_model_accuracy, clients_train_accuracies, clients_test_accuracies, filename=save_path)\n","    # After each round, reset callback state to prepare for the next round\n","    reset_callback_graph()\n","    print(f\"File saved to {save_path}\")\n","\n","#print(\"Accuracy data saved to\", csv_file_path)\n"]},{"cell_type":"code","source":["# Define the path to save global loss\n","global_loss_csv = '/content/drive/My Drive/Federated_Global_Loss_29_01_2025.csv'\n","\n","# Write headers to the CSV file (only at the beginning)\n","if epoch == 0:\n","    with open(global_loss_csv, mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow([\"Federated Round\", \"Global Loss\"])\n","\n","# Append the global loss after each round\n","with open(global_loss_csv, mode='a', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow([epoch, global_loss])\n","\n","    import matplotlib.pyplot as plt\n","\n","# Plot global loss over federated rounds\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(len(global_loss_per_round)), global_loss_per_round, marker='o', color='blue', label=\"Global Loss\")\n","plt.xlabel(\"Federated Round\")\n","plt.ylabel(\"Global Loss\")\n","plt.title(\"Global Loss Over Federated Rounds\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"WonID6yeY59G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Zs_rlJT5XJv"},"source":["Split data as iid and non-iid"]},{"cell_type":"markdown","metadata":{"id":"9a_q7UqBTcju"},"source":["new ways to average"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1zS-scE3TRhblZGCjmV4aOYzyblxPMaHc","timestamp":1736060136808},{"file_id":"1wzVu0QDWaGORXG5Ye_xJj4xO8ALG8Hud","timestamp":1734485228021},{"file_id":"1MefLVdbEMc-FEamBEekdnVsjaPFcV3QA","timestamp":1733113818972},{"file_id":"1StZk5RDM3Qjme5eUPpN0ryW87ndl8cod","timestamp":1731414758246},{"file_id":"17rAfjIC2usvGXyBR7HfqknWLlm_6yXkF","timestamp":1730290099203}],"mount_file_id":"1AHHRKY9AUF8ptCQtcVQs8vLyRva7Z_RC","authorship_tag":"ABX9TyProh88gKMlTclxBjZmdFnf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}